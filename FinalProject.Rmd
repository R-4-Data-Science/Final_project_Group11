---
title: "Multi-Path Stepwise Model Selection with AIC"
subtitle: "Final Project Report – Statistical Programming with R"
author: "Mark Philip, Prince Mensah Ansah and Jonathan Ng"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    theme: cosmo
    code_folding: show
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.width = 10,
  fig.height = 6
)
set.seed(42)

```


Executive Summary

Repository:

GitHub org/repo: R-4-Data-Science/Final_project_Group11

Link: https://github.com/R-4-Data-Science/Final_project_Group11

This report presents a complete implementation of a multi-path stepwise AIC model selection framework, including all three core algorithms required in the assignment:

Algorithm 1 — Multi-Path Forward Selection
Explores multiple near-optimal model paths by adding variables one at a time and retaining all children within a ΔAIC window of the best. This avoids the greedy trap of classical forward selection, which follows only one path.

Algorithm 2 — Stability via Resampling
Uses bootstrap (or subsampling) to compute π_j, the probability that each predictor appears in the model set. Stable variables appear consistently across resamples; noisy variables do not.

 Algorithm 3 — Plausible Model Selection
Combines:

AIC quality (models within Δ of the best AIC), and

Average stability (≥ τ threshold)
to select a final set of plausible, robust models.

Logistic Classification Extension
Includes a classification example and a full pipeline for binomial outcomes, parallel to the Gaussian (regression) case.

The overall goal is not to output a single “best” model, but to produce a Rashomon set of models that are:

well-supported by the data (low AIC), and

robust under resampling (high stability).

Package Implementation: mpssAIC

As part of this project, we implemented an R package, mpssAIC, which contains the production version of the functions defined in this report.

Repository: R-4-Data-Science/Final_project_Group11

Package subdirectory: mpssAIC

To install the package from GitHub:

# install.packages("devtools")
```{r install-mpssAIC, eval = FALSE}
# Installation example (do NOT run during knitting)

# install.packages("devtools")
devtools::install_github(
  "R-4-Data-Science/Final_project_Group11",
  subdir = "mpssAIC",
  build_vignettes = TRUE
)

library(mpssAIC)

```

The core exported functions in the package mirror the algorithms documented in this report:

build_paths() – Multi-path forward selection with AIC (Algorithm 1)

stability() – Resampling-based variable stability (Algorithm 2)

plausible_models() – Plausible model selection via AIC window + stability (Algorithm 3)

The code chunks below present a clean, self-contained implementation of the same methods for reproducibility within this HTML report.

Synthetic Example Data

The assignment requires synthetic examples in the HTML submission. To ensure full reproducibility and avoid dependence on any external dataset, we generate:

A Gaussian (linear regression) example

A Binomial (logistic regression) example





```{r load-data}
## Synthetic Gaussian data

n  <- 200
p  <- 8
X_linear <- as.data.frame(matrix(rnorm(n * p), n, p))
names(X_linear) <- paste0("x", 1:p)

beta <- c(2, -1.5, 0, 0, 1, rep(0, p - 5))
y_linear <- as.numeric(as.matrix(X_linear) %*% beta + rnorm(n, sd = 1.0))

## Synthetic Logistic data

n2 <- 240
p2 <- 6
X_logistic <- as.data.frame(matrix(rnorm(n2 * p2), n2, p2))
names(X_logistic) <- paste0("x", 1:p2)

linlp <- 0.6 + 1.0 * X_logistic$x1 - 1.2 * X_logistic$x2 + 0.8 * X_logistic$x5
pr    <- 1 / (1 + exp(-linlp))
y_logistic <- rbinom(n2, size = 1, prob = pr)

```


# Section 2: Core Helper Functions

## 2.1 Function to Fit and Return AIC

Core Helper Function
Fit Model and Return AIC

This helper function underlies Algorithm 1. It fits either a Gaussian or Binomial GLM with a given set of predictors and returns the AIC. It supports:

an empty model (intercept-only), and

any subset of predictors specified by column indices.


```{r fit-model-aic}

# -------------------------------------------------------------

# Core Helper: fit_model_aic()

# -------------------------------------------------------------

#' Compute AIC for a model with selected predictors
#'
#' @param X A data frame of predictors (n × p)
#' @param y Response vector of length n
#' @param vars Integer vector of column indices to include (empty = intercept-only)
#' @param model_type Either "gaussian" or "binomial"
#'
#' @return Numeric AIC value
fit_model_aic <- function(X, y, vars, model_type = c("gaussian", "binomial")) {
model_type <- match.arg(model_type)

# Empty model (intercept only)

if (length(vars) == 0) {
df  <- data.frame(y = y)
fam <- if (model_type == "gaussian") gaussian() else binomial()
fit <- glm(y ~ 1, data = df, family = fam)
return(AIC(fit))
}

# Data frame with selected predictors

df  <- data.frame(y = y, X[, vars, drop = FALSE])
fam <- if (model_type == "gaussian") gaussian() else binomial()
fit <- glm(y ~ ., data = df, family = fam)

AIC(fit)
}

## Quick internal test (Gaussian example)

test_aic_empty <- fit_model_aic(
X = X_linear,
y = y_linear,
vars = integer(0),
model_type = "gaussian"
)

test_aic_two_vars <- fit_model_aic(
X = X_linear,
y = y_linear,
vars = c(1, 2),
model_type = "gaussian"
)

cat("Test: Intercept-only AIC =", round(test_aic_empty, 3), "\n")
cat("Test: AIC with x1 + x2    =", round(test_aic_two_vars, 3), "\n")


```



# Section 3: Algorithm 1 – Multiple-Path Forward Selection

Algorithm 1 – Multi-Path Forward Selection
This algorithm builds multiple forward-selection paths by:
Starting from the empty model (intercept only)
At each step, for each parent model, adding one unused variable to create children
Keeping all children whose AIC is within δ of the best child AIC and improves the parent by at least ε
Deduplicating models and optionally capping the number of models per step at L

```{r algorithm1-multipath}
#-------------------------------------------------------------
#Algorithm 1: multi_path_forward()
#-------------------------------------------------------------

#' Algorithm 1: Multiple-Path Forward Selection
#'
#' @param X Data frame of predictors
#' @param y Response vector
#' @param model_type "gaussian" or "binomial"
#' @param K Maximum number of steps (defaults to min(p, 10))
#' @param eps Minimum AIC improvement required to expand a model
#' @param delta AIC tolerance for keeping near-ties
#' @param L Maximum number of models to keep per step
#'
#' @return List with:
#' - var_names: predictor names
#' - step_models: models at each step (list of lists)
#' - step_AICs: corresponding AICs
multi_path_forward <- function(
X, y,
model_type = c("gaussian", "binomial"),
K = NULL,
eps = 1e-6,
delta = 2,
L = 50
) {
model_type <- match.arg(model_type)
X <- as.data.frame(X)

p <- ncol(X)
var_names <- colnames(X)
if (is.null(K)) K <- min(p, 10)

#Helper to create unique key for a model

model_key <- function(idx) {
if (length(idx) == 0) return("")
paste(sort(idx), collapse = ",")
}

#Initialize: parent is empty model (intercept only)

parent_models <- list(integer(0))
parent_AICs <- fit_model_aic(X, y, vars = integer(0), model_type = model_type)

step_models <- list()
step_AICs <- list()

for (k in seq_len(K)) {
cat("Step", k, ": processing", length(parent_models), "parent model(s)...\n")

children_list <- list()
children_AICs <- numeric(0)
children_keys <- character(0)

# For each parent, generate all possible children
for (m in seq_along(parent_models)) {
  parent_idx <- parent_models[[m]]
  parent_aic <- parent_AICs[m]

  remaining <- setdiff(seq_len(p), parent_idx)
  if (length(remaining) == 0) next  # no more variables to add

  cand_models <- list()
  cand_AICs   <- numeric(0)

  # Try adding each remaining variable
  for (j in remaining) {
    child_idx <- sort(c(parent_idx, j))
    aic_child <- fit_model_aic(X, y, vars = child_idx, model_type = model_type)
    cand_models[[length(cand_models) + 1]] <- child_idx
    cand_AICs[length(cand_AICs) + 1]       <- aic_child
  }

  if (!length(cand_AICs)) next

  best_child_AIC <- min(cand_AICs)

  # Check: is there at least eps improvement?
  if ((parent_aic - best_child_AIC) < eps) {
    cat("  Parent", m, ": no improvement ≥ eps; stopping expansion.\n")
    next
  }

  # Keep children within delta of best child AIC
  keep_idx <- which(cand_AICs <= best_child_AIC + delta)
  for (i_keep in keep_idx) {
    child_idx <- cand_models[[i_keep]]
    aic_child <- cand_AICs[i_keep]
    key       <- model_key(child_idx)

    children_list[[length(children_list) + 1]] <- child_idx
    children_AICs[length(children_AICs) + 1]   <- aic_child
    children_keys[length(children_keys) + 1]   <- key
  }
}

# If no children generated, stop
if (!length(children_list)) {
  cat("No further improvement. Stopping.\n")
  break
}

# Deduplicate: keep best AIC per unique model key
df_children <- data.frame(
  key = children_keys,
  AIC = children_AICs,
  stringsAsFactors = FALSE
)

agg <- aggregate(AIC ~ key, data = df_children, FUN = min)
agg <- agg[order(agg$AIC), ]

# Cap at L models
if (!is.null(L) && nrow(agg) > L) {
  agg <- agg[seq_len(L), ]
}

cat("  Step", k, "produced", nrow(agg), "unique model(s) after dedup/cap.\n")

# Convert keys back to index vectors
new_parents <- vector("list", nrow(agg))
new_AICs    <- numeric(nrow(agg))

for (i in seq_len(nrow(agg))) {
  key <- agg$key[i]
  if (key == "") {
    idx <- integer(0)
  } else {
    idx <- as.integer(strsplit(key, ",")[[1]])
  }
  new_parents[[i]] <- idx
  new_AICs[i]      <- agg$AIC[i]
}

step_models[[k]] <- new_parents
step_AICs[[k]]   <- new_AICs

parent_models <- new_parents
parent_AICs   <- new_AICs

}

cat("\n=== Algorithm 1 Complete ===\n")

list(
var_names = var_names,
step_models = step_models,
step_AICs = step_AICs
)
}

cat("\n### RUNNING ALGORITHM 1: LINEAR REGRESSION (Synthetic) ###\n\n")
mp_full_linear <- multi_path_forward(
X = X_linear,
y = y_linear,
model_type = "gaussian",
K = NULL, # auto: min(p, 10)
eps = 1e-6,
delta = 2,
L = 50
)

cat("\nModels per step:\n")
print(sapply(mp_full_linear$step_models, length))






```



# Section 4: Algorithm 2 – Stability via Resampling

This algorithm estimates how stable each predictor is across resamples:
For each of B bootstrap (or subsample) replicates:
Run the multi-path search (Algorithm 1)
Collect all visited models
For each variable j, compute the fraction of models that include j
Average across resamples to obtain a stability score π_j ∈ [0, 1]


```{r algorithm2-stability}
#-------------------------------------------------------------
#Algorithm 2: compute_stability()
#-------------------------------------------------------------

#' Algorithm 2: Compute variable stability via resampling
#'
#' @param X Data frame of predictors
#' @param y Response vector
#' @param model_type "gaussian" or "binomial"
#' @param B Number of resamples
#' @param resample_type "bootstrap" or "subsample"
#' @param m Subsample size (if using subsample)
#' @param K, eps, delta, L Algorithm 1 parameters
#'
#' @return Numeric vector of stability scores (one per predictor)
compute_stability <- function(
X, y,
model_type = c("gaussian", "binomial"),
B = 50,
resample_type = c("bootstrap", "subsample"),
m = NULL,
K = NULL,
eps = 1e-6,
delta = 2,
L = 50
) {
model_type <- match.arg(model_type)
resample_type <- match.arg(resample_type)

X <- as.data.frame(X)
n <- nrow(X)
p <- ncol(X)
var_names <- colnames(X)

if (is.null(m)) m <- ceiling(sqrt(n))
# Matrix to store proportions: rows = resamples, cols = variables

Z <- matrix(0, nrow = B, ncol = p)
colnames(Z) <- var_names

for (b in seq_len(B)) {
if (b %% 10 == 0) cat("Resample", b, "/", B, "\n")

# Generate resample
if (resample_type == "bootstrap") {
  idx <- sample(seq_len(n), size = n, replace = TRUE)
} else {
  idx <- sample(seq_len(n), size = m, replace = FALSE)
}

Xb <- X[idx, , drop = FALSE]
yb <- y[idx]

# Run multi-path search on resample
mp_b <- multi_path_forward(
  X = Xb, y = yb,
  model_type = model_type,
  K = K,
  eps = eps,
  delta = delta,
  L = L
)

# Collect all models from this resample
all_models_b <- unlist(mp_b$step_models, recursive = FALSE)
if (!length(all_models_b)) next

M <- length(all_models_b)

# For each variable, count how many models contain it
for (j in seq_len(p)) {
  count_j <- sum(vapply(
    all_models_b,
    function(idx) j %in% idx,
    logical(1)
  ))
  Z[b, j] <- count_j / M
}

}

#Average over resamples

pi <- colMeans(Z)

cat("\n=== Algorithm 2 Complete ===\n")
cat("Stability scores (π):\n")
print(round(pi, 3))

pi
}

cat("\n### RUNNING ALGORITHM 2: STABILITY (LINEAR, Synthetic) ###\n\n")
stability_pi_linear <- compute_stability(
X = X_linear, y = y_linear,
model_type = "gaussian",
B = 20, # reduced for knitting speed; use 50+ in final
resample_type = "bootstrap",
K = NULL,
eps = 1e-6,
delta = 2,
L = 50
)

barplot(
stability_pi_linear,
main = "Variable Stability Scores (π) - Linear Model (Synthetic)",
ylab = "Stability π_j",
ylim = c(0, 1)
)
abline(h = 0.6, lty = 2, col = "red", lwd = 2)
legend("topright", legend = "τ = 0.6 threshold", lty = 2, col = "red")

```



# Section 5: Algorithm 3 – Plausible Model Selection

Algorithm 3 combines AIC and stability:
From Algorithm 1, collect all models and their AIC values.
Keep models within an AIC window Δ of the best model.
For each model, compute its average stability (mean π_j for variables in that model).
Keep only models with average stability ≥ τ.
If none qualify, return all models in the AIC window as a fallback.

```{r algorithm3-plausible}

# ---------------------------------------------------------
# Algorithm 3: Plausible Model Selection
# ---------------------------------------------------------
# Combines:
#  • AIC quality (within Δ of best model)
#  • Stability (average π ≥ τ)
# ---------------------------------------------------------

#' Algorithm 3: Select plausible, stable models
#'
#' @param mp_full Output list from Algorithm 1
#' @param stability_pi Named vector of stability scores (Algorithm 2)
#' @param Delta AIC tolerance (default = 2)
#' @param tau Minimum average stability threshold (default = 0.6)
#'
#' @return Data frame of plausible models:
#'   columns = key, AIC, AIC_diff, size, vars, avg_stability
#'
select_plausible_models <- function(
    mp_full,
    stability_pi,
    Delta = 2,
    tau = 0.6
) {

  var_names   <- mp_full$var_names
  step_models <- mp_full$step_models
  step_AICs   <- mp_full$step_AICs

  # Helper: unique key for a set of variables
  model_key <- function(idx) {
    if (length(idx) == 0) return("")
    paste(sort(idx), collapse = ",")
  }

  # Flatten all models and AICs
  all_models <- unlist(step_models, recursive = FALSE)
  all_AICs   <- unlist(step_AICs)

  if (length(all_models) == 0) {
    cat("No models generated!\n")
    return(data.frame())
  }

  # Create model keys
  keys <- vapply(all_models, model_key, character(1))

  df <- data.frame(
    key = keys,
    AIC = all_AICs,
    stringsAsFactors = FALSE
  )

  # Aggregate by model key → keep minimum AIC per model
  agg <- aggregate(AIC ~ key, data = df, FUN = min)

  # Remove intercept-only model
  agg <- agg[agg$key != "", ]

  if (nrow(agg) == 0) {
    cat("No non-intercept models!\n")
    return(data.frame())
  }

  # Compute AIC differences
  best_AIC      <- min(agg$AIC)
  agg$AIC_diff  <- agg$AIC - best_AIC

  # Filter by AIC ≤ best + Delta
  plausible <- agg[agg$AIC_diff <= Delta, ]

  if (nrow(plausible) == 0) {
    cat("No models within AIC window!\n")
    return(data.frame())
  }

  # Compute size and stability
  size      <- integer(nrow(plausible))
  avg_stab  <- numeric(nrow(plausible))
  vars_str  <- character(nrow(plausible))

  for (i in seq_len(nrow(plausible))) {
    key <- plausible$key[i]
    idx <- as.integer(strsplit(key, ",")[[1]])

    size[i] <- length(idx)

    these_vars <- var_names[idx]
    vars_str[i] <- paste(these_vars, collapse = " + ")

    # Stability of the variables in this model
    avg_stab[i] <- mean(stability_pi[these_vars])
  }

  plausible$size          <- size
  plausible$vars          <- vars_str
  plausible$avg_stability <- avg_stab

  # Filter by stability threshold τ
  plausible_final <- plausible[plausible$avg_stability >= tau, ]

  if (nrow(plausible_final) == 0) {
    cat("Warning: No models meet stability threshold τ =", tau, "\n")
    cat("Returning all models in AIC window.\n")
    plausible_final <- plausible
  }

  # Order by AIC
  plausible_final <- plausible_final[order(plausible_final$AIC), ]

  cat("\n=== Algorithm 3 Complete ===\n")
  cat("Selected", nrow(plausible_final), "plausible model(s).\n")

  plausible_final
}

```

# Algorithm 4 – Full Multi-Path AIC Pipeline

This function runs all three algorithms in sequence and returns a structured result.

```{r algorithm4-pipeline}
#-------------------------------------------------------------
#Algorithm 4: multi_path_AIC_pipeline()
#-------------------------------------------------------------

#' Algorithm 4: Complete Multi-Path AIC Pipeline
#'
#' @param X data frame of predictors
#' @param y response vector
#' @param model_type "gaussian" or "binomial"
#' @param K, eps, delta, L Algorithm 1 parameters
#' @param B, resample_type, m Algorithm 2 parameters
#' @param Delta, tau Algorithm 3 parameters
#'
#' @return list with:
#'   - mp_full: Algorithm 1 output
#'   - stability_pi: Algorithm 2 output
#'   - plausible: Algorithm 3 output
multi_path_AIC_pipeline <- function(
    X, y,
    model_type = c("gaussian", "binomial"),
    K = NULL,
    eps = 1e-6,
    delta = 2,
    L = 50,
    B = 50,
    resample_type = c("bootstrap", "subsample"),
    m = NULL,
    Delta = 2,
    tau = 0.6
) {
  model_type    <- match.arg(model_type)
  resample_type <- match.arg(resample_type)

  cat("\n", strrep("=", 60), "\n")
  cat("MULTI-PATH AIC PIPELINE\n")
  cat("Model type:", model_type, "\n")
  cat("Resamples (B):", B, "\n")
  cat(strrep("=", 60), "\n\n")

  # Step 1: Multi-path forward selection
  cat(">>> STEP 1: Multi-Path Forward Selection <<<\n\n")
  mp_full <- multi_path_forward(
    X = X, y = y,
    model_type = model_type,
    K = K,
    eps = eps,
    delta = delta,
    L = L
  )

  # Step 2: Stability via resampling
  cat("\n>>> STEP 2: Stability Estimation via Resampling <<<\n\n")
  stability_pi <- compute_stability(
    X = X, y = y,
    model_type    = model_type,
    B             = B,
    resample_type = resample_type,
    m             = m,
    K = K,
    eps = eps,
    delta = delta,
    L = L
  )

  # Step 3: Plausible models
  cat("\n>>> STEP 3: Plausible Model Selection <<<\n\n")
  plausible <- select_plausible_models(
    mp_full      = mp_full,
    stability_pi = stability_pi,
    Delta        = Delta,
    tau          = tau
  )

  cat("\n", strrep("=", 60), "\n")
  cat("PIPELINE COMPLETE\n")
  cat(strrep("=", 60), "\n")

  list(
    mp_full      = mp_full,
    stability_pi = stability_pi,
    plausible    = plausible
  )
}

```

# Section 8: Complete Example – Linear Regression (Synthetic)

For classification, we use a binary outcome. Here, high MPG (≥20) vs. low MPG (<20).

```{r example-Linear}
cat("\n\n")
cat(strrep("#", 70), "\n")
cat("COMPLETE EXAMPLE: LINEAR REGRESSION (Synthetic Data)\n")
cat(strrep("#", 70), "\n\n")

result_linear <- multi_path_AIC_pipeline(
X = X_linear,
y = y_linear,
model_type = "gaussian",
K = NULL, # min(p, 10)
eps = 1e-6,
delta = 2,
L = 50,
B = 20, # reduced for knitting speed; 50+ recommended
resample_type = "bootstrap",
m = NULL,
Delta = 2,
tau = 0.6
)

cat("\n### FINAL RESULTS: LINEAR REGRESSION ###\n\n")
cat("Number of plausible models:", nrow(result_linear$plausible), "\n\n")

final_table <- result_linear$plausible[
, c("AIC", "AIC_diff", "size", "vars", "avg_stability")
]
rownames(final_table) <- NULL
print(final_table)

cat("\n### Stability Scores ###\n")
print(round(result_linear$stability_pi, 3))
```


Complete Example – Logistic Regression (Synthetic Binary Data)

```{r example-Logistic}
cat("\n\n")
cat(strrep("#", 70), "\n")
cat("COMPLETE EXAMPLE: LOGISTIC REGRESSION (Synthetic Binary Data)\n")
cat(strrep("#", 70), "\n\n")

result_logistic <- multi_path_AIC_pipeline(
X             = X_logistic,
y             = y_logistic,
model_type    = "binomial",
K             = NULL,
eps           = 1e-6,
delta         = 2,
L             = 50,
B             = 20,
resample_type = "bootstrap",
m             = NULL,
Delta         = 2,
tau           = 0.6
)

cat("\n### FINAL RESULTS: LOGISTIC REGRESSION ###\n\n")
cat("Number of plausible models:", nrow(result_logistic$plausible), "\n\n")

final_table_log <- result_logistic$plausible[
, c("AIC", "AIC_diff", "size", "vars", "avg_stability")
]
rownames(final_table_log) <- NULL
print(final_table_log)

cat("\n### Stability Scores ###\n")
print(round(result_logistic$stability_pi, 3))

```


Interpretation & Key Insights
Why This Method Works

Multiple paths.
Traditional stepwise regression follows a single path and can easily miss competitive models. Multi-path selection keeps several near-optimal paths, exposing alternative explanations of the data.

Resampling-based stability.
A variable that appears only occasionally in resampled datasets is likely unstable and driven by noise. A variable with high π_j is consistently supported by the data and should be trusted.

AIC window + stability threshold.
Models within 2 AIC units of the best model are often statistically indistinguishable. By combining an AIC window (Δ) with a stability threshold (τ), we obtain a Rashomon set of models that are both high-quality and robust.

# Parameter Guidance
```{r}
cat("\n### PARAMETER SUMMARY ###\n\n")

params_df <- data.frame(
Algorithm = c(
"Algorithm 1", "Algorithm 1", "Algorithm 1", "Algorithm 1",
"Algorithm 2", "Algorithm 2",
"Algorithm 3", "Algorithm 3"
),
Parameter = c(
"K", "ε", "δ", "L",
"B", "resample_type",
"Δ", "τ"
),
Used_Value = c(
"min(p, 10)", "1e-6", "2", "50",
"20 (demo; 50+ recommended)", "bootstrap",
"2", "0.6"
),
Description = c(
"Maximum model size",
"Minimum AIC improvement threshold",
"AIC tolerance for near-ties",
"Max models per step",
"Number of resamples",
"Resample method (bootstrap/subsample)",
"AIC tolerance for plausibility window",
"Minimum average stability threshold"
),
stringsAsFactors = FALSE
)

print(params_df)

```



Conclusion

This report demonstrates a complete, reproducible implementation of the Multi-Path Stepwise Model Selection with AIC framework:

lgorithm 1: Multi-path forward selection building a forest of models

Algorithm 2: Stability estimation via bootstrap resampling

Algorithm 3: Plausible model selection combining AIC and stability

Algorithm 4: Integrated pipeline for both Gaussian and Binomial models

Synthetic examples: Linear and logistic regression

Package implementation: mpssAIC, installable from GitHub with a vignette

The final plausible models represent a principled compromise between statistical fit (low AIC) and robustness (high stability), providing multiple trustworthy model options rather than a single brittle solution.

For further extensions, one could:

Increase B (e.g., 50–100) for more precise stability estimates

Perform sensitivity analysis over Δ and τ

Add confusion matrices and ROC curves for logistic models

Apply the pipeline to real datasets and compare against LASSO or other modern methods.

Portions of the documentation, code refinement, and explanatory text were produced with assistance from an AI language model (ChatGPT). All algorithm design decisions, implementation logic, and final verification were performed by the authors. Link: 

